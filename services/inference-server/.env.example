# Mythic Inference Server Configuration
# ============================================

# L2 RPC endpoint (your local validator or rpc.mythic.sh)
L2_RPC_URL=http://localhost:8899
L2_WS_URL=ws://localhost:8900

# Validator keypair (JSON array format or path to .json file)
VALIDATOR_KEYPAIR_PATH=./validator-keypair.json

# Stake amount in lamports (minimum required by AI Precompiles program)
STAKE_AMOUNT=1000000000

# AI Precompiles program ID
AI_PROGRAM_ID=CT1yUSX8n5uid5PyrPYnoG5H6Pp2GoqYGEKmMehq3uWJ

# GPU configuration
GPU_MODEL=NVIDIA RTX 4090
VRAM_GB=24

# Model storage directory
MODEL_DIR=./models

# Default model to load (GGUF format for llama.cpp)
DEFAULT_MODEL=llama-3.1-8b-instruct.Q4_K_M.gguf

# Inference server HTTP port (for health checks + local API)
HTTP_PORT=8080

# Maximum concurrent inference requests
MAX_CONCURRENT=4

# Supported model hashes (comma-separated SHA256 hashes of model weights)
# Leave empty to support all registered models
SUPPORTED_MODELS=

# Log level: debug, info, warn, error
LOG_LEVEL=info

# Gateway API URL (to announce availability)
GATEWAY_URL=https://ai.mythic.sh
